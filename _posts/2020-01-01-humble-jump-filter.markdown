---
layout: post
title: "Humble Jump Filter"
date: 2020-01-01 13:10:00 -0500
categories: Filters
---

_A humble methodology to filter jumps from price returns observations using maximum likelihood._

---
<br/>
# 1. Introduction

The starting point for modeling a series of price returns $$(r_t)_{t=1}^T$$ is to assume they are i.i.d. [Gaussian random variables][gaussian-dist] with mean $$\mu$$ and standard deviation $$\sigma$$. In other words, they may be written as

$$r_t = \mu + u_t,\qquad t\in\{1,\ldots,T\},$$

where each $$u_t$$ is a centered Gaussian random variable with standard deviation $$\sigma$$.

Although this assumption is very convenient from an operational perspective -- for example, the sum of two returns is also Gaussian -- one might be interested in allowing the price returns to **jump**. Why? Because it is a natural way to represent large movements driven by major news shocks. An easy way do this is to add a jump term to the previous model and write

$$r_t = \mu + u_t + z_t\xi_t,\qquad t\in\{1,\ldots,T\},$$

where we also assume each $$z_t$$ follows a Gaussian distribution with mean $$\mu_J$$ and standard deviation $$\sigma_J$$, while $$\xi_t$$ follows a [Bernoulli distribution][bernoulli-dist] with parameter $$\lambda$$. For simplicity, we will also assume that the $$\epsilon$$ and $$z$$ are two independent processes. Therefore, the vector containing our model parameters is given by

$$\boldsymbol{\Theta} = \begin{bmatrix}
\mu & \sigma & \lambda & \mu_J & \sigma_J
\end{bmatrix}^\intercal.$$

Two observations must be made. First, the model presented here actually corresponds to a discrete-time version of the model introduced by [Merton (1976)][merton-76]. 

Second, notice that when there is no jump, ie. $$\xi_t= 0$$, we recover the original expression for the price return. However, when the price jumps, ie. $$\xi_t= 1$$, the variable of interest now corresponds to the sum of two independent Gaussian random variables. In other words, we may interpret this model as if the price returns were sampled with probability $$1 - \lambda$$ from a Gaussian distribution with parameters $$(\mu, \sigma^2)$$ and with probability $$\lambda$$, with parameters $$(\mu+\mu_J, \sigma^2+\sigma^2_J)$$. This type of model is sometimes called a [Mixture Model][mixture-model].

# 2. Calibration

The model introduced can be easily calibrated using the **maximum likelihood estimation** ([MLE][mle]) technique. Denote by $$\boldsymbol{r}^T$$ the vector of price returns observations until $$T$$. Given a set of parameters $$\boldsymbol{\Theta}$$, the definition of conditional probability allows us to write the likelihood of said observations as

$$p\left(\boldsymbol{r}^T \;\middle|\; \boldsymbol{\Theta}\right)
= p\left(r_T, \boldsymbol{r}^{T-1} \;\middle|\; \boldsymbol{\Theta}\right)
= p\left(r_T \;\middle|\; \boldsymbol{r}^{T-1}, \boldsymbol{\Theta}\right) p\left(\boldsymbol{r}^{T-1} \;\middle|\; \boldsymbol{\Theta}\right).
$$

After applying this relation recursively and taking logarithms, we may define the log-likelihood function as

$$L\left(\boldsymbol{\Theta}\right) = \log p\left(\boldsymbol{r}^T \;\middle|\; \boldsymbol{\Theta}\right)
= \log\prod_{t=1} ^ T p\left(r_t \;\middle|\; \boldsymbol{r}^{t-1}, \boldsymbol{\Theta}\right)
= \sum_{t=1} ^ T \log p\left(r_t \;\middle|\; \boldsymbol{r}^{t-1}, \boldsymbol{\Theta}\right).
$$

Additionally, our model is so humble that, given a set of parameters, the price return today is independent from the past returns.[^humble] Hence, we may simply write

$$
p\left(r_t \;\middle|\; \boldsymbol{r}^{t-1}, \boldsymbol{\Theta}\right)
= p\left(r_t \;\middle|\; \boldsymbol{\Theta}\right)
= (1-\lambda)g\left(r_t \;\middle|\; \mu, \sigma^2\right) + \lambda g\left(r_t \;\middle|\; \mu + \mu_J, \sigma^2 + \sigma_J^2\right),
$$

where $$g$$ is the normal probability density of a Gaussian variable given by

$$
g\left(r_t \;\middle|\; \mu, \sigma^2\right)
= \frac 1{\sqrt{2\pi\sigma^2}}\exp\left(- \frac{(x - \mu)^2}{2\sigma^2}\right).
$$

Based on these expression, the MLE estimator for the parameters vector $$\boldsymbol{\hat\Theta}$$ is obtained by maximizing the log-likelihood function $$L$$.

Lastly, once the parameters have been estimated, we may compute the probability that a return was actually generated by a jump. Simply notice the Bayes theorem yields that

$$p\left(\xi_t\;\middle|\; r_t, \boldsymbol{\Theta}\right) = \frac{p\left(r_t \;\middle|\; \xi_t, \boldsymbol{\Theta}\right)p\left(\xi_t\;\middle|\; \boldsymbol{\Theta}\right)}{p\left(r_t \;\middle|\; \boldsymbol{\Theta}\right)},$$

and all the expressions on the right-hand side of this equation were computed above.

# 3. Numerical Experiment

After all these equations, its straightforward to say "Well, does it work?" To answer this, we may run a numerical experiment simulating pseudo-random numbers to evaluate the performance of the proposed filter.

I did this generating 10,000 observations from a set of parameters that seemed reasonable for daily returns. Specifically, this values imply a mean return (in absence of jumps) of 5% per annum with an annual volatility of 20%. Additionally, 5% of the days are expected to be shocked by a Gaussian variable with mean -6% and volatility 3%. Moreover, as it may be seen from the following histogram, at least at first sight, one might believe these returns come from an actual asset -- perhaps a volatility stock?

![hist](/assets/hist.png)

The results of the estimation are presented in the following table.[^stderr] As it may be seen, the estimated values are quite close to the actual ones and within two -- or even one -- standard deviations. Pretty decent.[^return]

| Parameter	    | Actual 	| Estimated | StErr 	|
|:------:	    |:------:	|:------:	|:------:	|
| $$\mu$$     	|  0.020 	|  0.017 	|  0.014 	|
| $$\sigma$$  	|  1.260 	|  1.265 	|  0.010	|
| $$\lambda$$ 	|  0.050 	|  0.046 	|  0.004 	|
| $$\mu_J$$    	| -6.000 	| -6.118	|  0.358 	|
| $$\sigma_J$$ 	|  3.000 	|  3.037 	|  0.257 	|

Now, if we identify a jump only when its probability is greater than 50%, how many of the actual jumps did the filter capture? To answer this, let's look at the [confusion matrix][confMatrix]. From it we may read that most of the no-jump movements were correctly identified as such. Meanwhile, from 469 actual jumps, only 352 were correctly labeled as such while 117 were not identified. Is this a bad thing? 

|  Predicted vs. Actual | No Jump | Jump |  Total |
|:---------------------:|:-------:|:----:|:------:|
|**No Jump**            |  9,515  |  117 |  9,632 |
|**Jump**               |    16   |  352 |    368 |
|**Total**              |  9,531  |  469 | 10,000 |

Not necessarily. The following plot displays the simulated returns label accordingly with their actual nature (no jump or jump) vs. the probability they will be identified as jumps by the filter. What happened was that only the large movements -- those that would appear on the newspapers front page -- are identified as jumps. Notice that, eventually, a large positive return could also come from one. Sometimes the jump occurred but wasn't large enough to move the return dramatically away from zero. On the other hand, observe that the 16 dark points identified as jumps, were quite strong plummets. Lastly, after a certain level, the filter identifies jumps almost surely.

![inferredJumps](/assets/inferredJumps.png)

---

[^humble]: The humbleness of the models is remarked because there is a vast set more complex models and estimation techniques.
[^stderr]: The standard errors were computed using the popular *sandwich* estimator proposed by White involving the Hessian and the outer product of the gradients.
[^return]: Although, the estimated mean return $$\hat\mu$$ ends up being non-statistically different from zero -- as usual.

[gaussian-dist]: https://en.wikipedia.org/wiki/Normal_distribution
[bernoulli-dist]: https://en.wikipedia.org/wiki/Bernoulli_distribution
[mixture-model]: https://en.wikipedia.org/wiki/Mixture_model
[mle]: https://en.wikipedia.org/wiki/Maximum_likelihood_estimation
[confMatrix]: https://en.wikipedia.org/wiki/Confusion_matrix
[merton-76]: https://www.sciencedirect.com/science/article/abs/pii/0304405X76900222?via%3Dihub